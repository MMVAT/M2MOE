# ğŸš€ Modality-Specific and Modality-Interaction Enhanced Mixture of Experts for Imbalanced Audio-Visual Video Parsing Learning
This is the official code for the Modality-Specific and Modality-Interaction Enhanced Mixture of Experts for Imbalanced Audio-Visual Video Parsing Learning.

![image](https://github.com/MMVAT/M2MOE/blob/main/arch.png?raw=true)


# ğŸ’» Machine environment
- Ubuntu version: 20.04.6 LTS (Focal Fossa)
- CUDA version: 12.2
- PyTorch: 1.12.1
- Python: 3.10.12
- GPU: NVIDIA A100-SXM4-40GB
- [![Python](https://img.shields.io/badge/Python-3.10+-blue.svg)](https://www.python.org/)
  [![PyTorch](https://img.shields.io/badge/PyTorch-1.12+-orange.svg)](https://pytorch.org/)
  [![CUDA](https://img.shields.io/badge/CUDA-11.2+-green.svg)](https://developer.nvidia.com/cuda-zone)

# ğŸ›  Environment Setup
1. Clone the repository:
```bash
git clone https://github.com/yourusername/M2MOE.git
cd M2MOE
```

2. Create and activate the conda environment:
```bash
conda env create -f environment.yaml
conda activate m2moe
```

# ğŸ“‚ Data Preparation
### Annotation files
Please download LLP dataset annotations (6 CSV files) from [AVVP-ECCV20](https://github.com/YapengTian/AVVP-ECCV20) and place them in ```data/r2plus1d_18/.```

### CLAP- & CLIP-extracted features
Please download the CLAP-extracted features (CLAP.7z) and CLIP-extracted features (CLIP.7z) from [this link](https://pan.quark.cn/s/db27c79f651b?pwd=rF5C), unzip the two files, and place the decompressed CLAP-related files in ```data/CLAP/.``` and the CLIP-related files in ```data/CLIP/.```

### File structure for datasets
Please make sure that the file structure is the same as the following.
```bash
data/                                
â”‚   â”œâ”€â”€ AVVP_dataset_full.csv               
â”‚   â”œâ”€â”€ AVVP_eval_audio.csv             
â”‚   â”œâ”€â”€ AVVP_eval_visual.csv                 
â”‚   â”œâ”€â”€ AVVP_test_pd.csv                
â”‚   â”œâ”€â”€ AVVP_train.csv                     
â”‚   â”œâ”€â”€ AVVP_val_pd.csv                      
â”‚   â”œâ”€â”€ CLIP/                                
â”‚   â”‚   â”œâ”€â”€ features/        
â”‚   â”‚   â”‚   â”œâ”€â”€ -00BDwKBD5i8.npy
â”‚   â”‚   â”‚   â”œâ”€â”€ -00fs8Gpipss.npy
â”‚   â”‚   â”‚   â””â”€â”€ ... 
â”‚   â”‚   â”œâ”€â”€ segment_pseudo_labels/        
â”‚   â”‚   â”‚   â”œâ”€â”€ -00BDwKBD5i8.npy
â”‚   â”‚   â”‚   â”œâ”€â”€ -00fs8Gpipss.npy
â”‚   â”‚   â”‚   â””â”€â”€ ... 
â”‚   â”œâ”€â”€ CLAP/              
â”‚   â”‚   â”œâ”€â”€ features/        
â”‚   â”‚   â”‚   â”œâ”€â”€ -00BDwKBD5i8.npy
â”‚   â”‚   â”‚   â”œâ”€â”€ -00fs8Gpipss.npy
â”‚   â”‚   â”‚   â””â”€â”€ ... 
â”‚   â”‚   â”œâ”€â”€ segment_pseudo_labels/        
â”‚   â”‚   â”‚   â”œâ”€â”€ -00BDwKBD5i8.npy
â”‚   â”‚   â”‚   â”œâ”€â”€ -00fs8Gpipss.npy
â”‚   â”‚   â”‚   â””â”€â”€ ... 
â”‚   â”œâ”€â”€ r2plus1d_18/              
â”‚   â”‚   â”œâ”€â”€ -00BDwKBD5i8.npy
â”‚   â”‚   â”œâ”€â”€ -00fs8Gpipss.npy
â”‚   â”‚   â””â”€â”€ ... 
```

# ğŸ“ Download trained models
Please download the trained models from [this link](https://pan.quark.cn/s/f9f220c0e73d?pwd=mRZ2) and put the models in their corresponding model directory.

# ğŸ”¥ Training and Inference
We provide bash file for a quick start.
#### For Training
```
bash train.sh
```

#### For Inference
```
bash test.sh
```

# ğŸ¤ Contributing
We welcome contributions to the M2MOE project! To contribute:

1. **Fork the repository** and create a new branch for your feature
2. **Ensure your code follows the project's coding standards**
3. **Add tests** for any new functionality
4. **Submit a pull request** with a clear description of your changes

# ğŸ“ Contact & Acknowledgements

## Contact Information

- **Primary Contact**: [isahini@csuft.edu.cn]
- **GitHub Issues**: [Submit issues](https://github.com/MMVAT/M2MOE/issues)
- **Discussions**: [Join discussions](https://github.com/MMVAT/M2MOE/discussions)

## ğŸ¤ Acknowledgements

We extend our gratitude to the following organizations and research groups:

- **LAION Team** for providing the CLAP audio-visual pre-trained models
- **AVVP Dataset Contributors** for the comprehensive audio-visual event dataset
- **PyTorch Team** for the robust deep learning framework
- **Open Source Community** for the invaluable contributions to multi-modal learning research

---

<div align="center">

**If this project contributes to your research, please consider giving it a â­ Star!**

</div>
